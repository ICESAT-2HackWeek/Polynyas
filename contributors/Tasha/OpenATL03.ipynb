{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428d948-a637-4ab4-aca2-48c97724e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''From Facu Sapienza, Hackweek 2021'''\n",
    "\n",
    "from icepyx import icesat2data as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "###\n",
    "\n",
    "# To do:\n",
    "#       - Remove the emails and do this in a more general way\n",
    "#       - Use projections in the definition of delta_lat and delta_lon instead of the spherical approximation\n",
    "\n",
    "###\n",
    "\n",
    "earthdata_emails = {'tsnow03':'tasha.snow@colorado.edu', \\}\n",
    "\n",
    "user = \"tsnow03\"\n",
    "\n",
    "### Auxiliar Functions\n",
    "\n",
    "def delta_lat(lat, lon, delta_m):\n",
    "    return 180 * delta_m / ( np.pi * 6371000 )\n",
    "\n",
    "def delta_lon(lat, lon, delta_m):\n",
    "    return 180 * delta_m / ( np.pi * 6371000 * np.cos(lat * np.pi / 180) )\n",
    "\n",
    "def filter(string, substr): \n",
    "    return [str for str in string if\n",
    "             any(sub in str for sub in substr)] \n",
    "\n",
    "def df_filter (df, my_lat, my_lon, w, lat_col_name = \"lat\", lon_col_name = \"lon\"):\n",
    "    \n",
    "    window_lat = delta_lat(my_lat, my_lon, w)\n",
    "    window_lon = delta_lon(my_lat, my_lon, w) \n",
    "    \n",
    "    return df [ (df[lat_col_name] < my_lat + window_lat) & (df[lon_col_name] < my_lon + window_lon) &\n",
    "                (df[lat_col_name] > my_lat - window_lat) & (df[lon_col_name] > my_lon - window_lon) ]\n",
    "\n",
    "def file_in_dir(path):\n",
    "    return [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "\n",
    "### ATL03 Retrieval\n",
    "\n",
    "def read_atl03 (lat, lon, date_range, delta_m, path = \"new_ATL03\", extent = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read a ATL03 file based and retieve individual photons in a window around a\n",
    "    desired latitide, longitud and a range of dates. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Spatial extend\n",
    "    \n",
    "    if extent == None:\n",
    "\n",
    "        window_lat = delta_lat(lat, lon, delta_m)\n",
    "        window_lon = delta_lon(lat, lon, delta_m)\n",
    "    \n",
    "        spatial_extent = [ lon - window_lon, lat - window_lat, lon + window_lon, lat + window_lat ]\n",
    "    \n",
    "    else:\n",
    "\n",
    "        spatial_extent = extent \n",
    "    \n",
    "    spatial_extent = [ float(x) for x in spatial_extent ]  # This line has to be remove after solving Issue 82 in Icepyx\n",
    "    \n",
    "    # Retreiving the data \n",
    "    \n",
    "    region_a = ipd.Icesat2Data('ATL03', spatial_extent, date_range)\n",
    "    region_a.avail_granules(ids=True)\n",
    "    region_a.earthdata_login(user, earthdata_emails[user])\n",
    "    region_a.order_vars.append(var_list=['lat_ph', \"lon_ph\", \"h_ph\"])\n",
    "    region_a.subsetparams(Coverage=region_a.order_vars.wanted)\n",
    "    region_a.order_granules()\n",
    "    \n",
    "    region_a.download_granules(path)\n",
    "        \n",
    "    flist = file_in_dir(path)\n",
    "    assert len(flist) > 0, \"There are not available granules for these parameters. Check that the h5 files were download in path\"\n",
    "        \n",
    "        \n",
    "    dataframes = pd.DataFrame(columns = [\"h_ph\", \"lon_ph\", \"lat_ph\", \"ground_track\"])\n",
    "    \n",
    "    for file in flist:\n",
    "        \n",
    "        fname = path + \"/\" + file \n",
    "        \n",
    "        with h5py.File(fname, 'r') as fi: \n",
    "            \n",
    "            for my_gt in filter(fi.keys(), [\"gt\"]):\n",
    "    \n",
    "                lat_ph = fi[my_gt]['heights'][\"lat_ph\"][:]\n",
    "                lon_ph = fi[my_gt]['heights'][\"lon_ph\"][:]\n",
    "                h_ph   = fi[my_gt]['heights'][\"h_ph\"][:]\n",
    "\n",
    "                \n",
    "                df = pd.DataFrame.from_dict({\"h_ph\": h_ph,\n",
    "                                             \"lon_ph\": lon_ph,\n",
    "                                             \"lat_ph\": lat_ph,\n",
    "                                             \"ground_track\": [my_gt] * len(h_ph) } )\n",
    "    \n",
    "                if extent == None:\n",
    "\n",
    "                    df = df [ (df[\"lat_ph\"] < lat + window_lat) & (df[\"lon_ph\"] < lon + window_lon) &\n",
    "                              (df[\"lat_ph\"] > lat - window_lat) & (df[\"lon_ph\"] > lon - window_lon) ]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    df = df [ (df[\"lat_ph\"] < extent[3]) & (df[\"lon_ph\"] < extent[2]) &\n",
    "                              (df[\"lat_ph\"] > extent[1]) & (df[\"lon_ph\"] > extent[0]) ]\n",
    "\n",
    "                dataframes = dataframes.append(df, ignore_index=True)\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "\n",
    "\n",
    "def multiple_read_atl03 (requests, earthdata_email = \"fsapienza@berkeley.edu\", earthdata_uid = \"fsapienza\", delta_m = 100):\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    for i, req in enumerate(requests): \n",
    "        \n",
    "        df = read_atl03( lat = req[\"lat\"], lon = req[\"lon\"], date_range = req[\"date_range\"], delta_m = delta_m, path = \"new_ATL03/file\" + str(i+1) )\n",
    "        \n",
    "        res[i] = df\n",
    "        \n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
