{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613b0da5-0bc7-4133-8c44-efb28915ab98",
   "metadata": {},
   "source": [
    "# Load, Read and Analyze ERA5 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2de76-d94f-48c3-9256-e6c9b9758211",
   "metadata": {},
   "source": [
    "First, import some required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f962285-b678-4da3-8ef2-64cff2783fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import ee\n",
    "import geemap\n",
    "import requests\n",
    "import cftime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import datetime as dt\n",
    "import rasterio as rio\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.util as cutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from rasterio import plot\n",
    "from rasterio import warp\n",
    "from shapely import geometry\n",
    "from datetime import timedelta\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from cartopy.io.img_tiles import Stamen\n",
    "from ipyleaflet import basemaps, basemap_to_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fdf4e-8677-4740-9752-9c8e79495508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_read(filename, variable):\n",
    "    '''\n",
    "    Read variable data from a NetCDF source\n",
    "    :param filename:(string) complete path and file name\n",
    "    :param variable: (string) name of variable\n",
    "    :return: numpy array containing data\n",
    "    '''\n",
    "    \n",
    "    data = nc.Dataset(filename)\n",
    "    var = np.squeeze(data[variable][:])\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323bb90-beec-4aa1-b733-e04c297c83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_variable_data(file, var, t_units):\n",
    "    '''\n",
    "    Return the time, latitude, longitude and variable values for the netcdf4 file\n",
    "    :param file: (string) the complete path and file name\n",
    "    :param var: (string) the variable name as defined in the NetCDF4 file.\n",
    "    :param t_units: (string) the units for the time\n",
    "    :return time: numpy array containing time data\n",
    "    :return lats: numpy array containing latitude data\n",
    "    :return lons: numpy array containing longitude data\n",
    "    :return var: numpy array containing data for the variable of interest\n",
    "    '''\n",
    "    time = nc_read(file, 'time')\n",
    "    lats = nc_read(file, 'latitude')\n",
    "    lons = nc_read(file , 'longitude')\n",
    "    var = nc_read(file, var)\n",
    "\n",
    "    time.units = t_units\n",
    "    time = nc.num2date(time, time.units)\n",
    "    \n",
    "    return time, lats, lons, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947297ed-aa97-4d62-a705-cc6146941227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_extent_reduction(lat, lon, var):\n",
    "    '''\n",
    "    Reduces the variable data to the Pine Island Glacier Polynya region.\n",
    "    :param lat: numpy array containing original latitude data\n",
    "    :param lon: numpy array containing original longitude data\n",
    "    :param lat: numpy array containing original variable data\n",
    "    :return: The same input variable for the reduced region\n",
    "    '''\n",
    "    lat = lat[3:6]\n",
    "    lon = lon[:-2]\n",
    "    var = var[:,:,3:6,:-2]\n",
    "    return lat, lon, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431212e-4b18-4b3c-b002-e687465ea891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(var, spatial_reduction=0):\n",
    "    '''\n",
    "    File that loads the desired ERA5 data, and reduces the spatial extent if required.\n",
    "    :param var: (string) variable name of interest\n",
    "    :param spatial_reduction: (int) Can take values 0 or 1 where there is no spatial reduction for 0 and spatial reduction for 1\n",
    "    :return: Processed ERA5 data for the specified variable of interest\n",
    "    '''\n",
    "    file = '/home/jovyan/Polynyas/data/ERA5/PineIslandPolynya_ERA5_data.nc'\n",
    "    t_units = 'hours since 1900-01-01'\n",
    "    time, lat, lon, var = return_variable_data(file, var, t_units)\n",
    "    if spatial_reduction ==1:\n",
    "        lat, lon, var = spatial_extent_reduction(lat, lon, var)\n",
    "        return time, lat, lon, var\n",
    "    else:\n",
    "        return time, lat, lon, var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b3085-6064-4eec-8d46-83c5900b5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_visualisation(vardata, lon, lat, varname, date, filled_contours=0):\n",
    "    '''\n",
    "    Creates a contour plot for the variable data of interest.\n",
    "    :param vardata: numpy array containing data for the variable of interest.\n",
    "    :param lon: 1D numpy array containing longitude data \n",
    "    :param lat: 1D numpy array containing latitude data \n",
    "    :param varname: (string) the variable name for plotting\n",
    "    :param date: (string) the time period spanned by the variable data\n",
    "    :param filled_contours: set to 0 if no filled contours, set to 1 for filled contours\n",
    "    '''\n",
    "    \n",
    "    lons, lats = np.meshgrid(lon, lat)\n",
    "    plt.style.use('seaborn-bright')\n",
    "    tiler = Stamen('terrain-background')\n",
    "    mercator = tiler.crs\n",
    "\n",
    "    fig = plt.figure(figsize=[14,14])\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=mercator)\n",
    "    \n",
    "    ax.set_extent([ -100, -102, -75.75, -74.25], ccrs.PlateCarree())\n",
    "\n",
    "    if filled_contours==0:\n",
    "        print(\"no filled contours\")\n",
    "    elif filled_contours==1:\n",
    "        filled_c = ax.contourf(lons, lats, vardata,transform=ccrs.PlateCarree(),cmap='nipy_spectral')\n",
    "        fig.colorbar(filled_c, orientation='horizontal')\n",
    "    \n",
    "    line_c = ax.contour(lons, lats, vardata, colors=['black'],transform=ccrs.PlateCarree())\n",
    "    ax.clabel(line_c, colors=['black'], manual=False, inline=True, fmt=' {:.0f} '.format)\n",
    "    geom = geometry.box(minx=-101.8,maxx=-100.5,miny=-75.25,maxy=-74.8)\n",
    "    ax.add_geometries([geom], crs=ccrs.PlateCarree(), facecolor='r', edgecolor='black', alpha=0.3)\n",
    "    ax.add_image(tiler, 6)\n",
    "    ax.coastlines()\n",
    "    ax.set_title('Quick Visualisation of {}'.format(varname)+'\\n for {}'.format(date), fontsize=20, fontweight='bold', pad=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f25612-6932-40bd-9f44-75941bc53f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_visualisation_winds(u, v, lon, lat, varname, date):\n",
    "    '''\n",
    "    Creates a vector plot for the 10m wind.\n",
    "    :param u: numpy array containing zonal wind component data\n",
    "    :param v: numpy array containing meridional wind component data\n",
    "    :param lon: 1D numpy array containing longitude data \n",
    "    :param lat: 1D numpy array containing latitude data \n",
    "    :param varname: (string) the variable name for plotting\n",
    "    :param date: (string) the time period spanned by the variable data\n",
    "    '''\n",
    "    lons, lats = np.meshgrid(lon, lat)\n",
    "    plt.style.use('seaborn-bright')\n",
    "    tiler = Stamen('terrain-background')\n",
    "    mercator = tiler.crs\n",
    "\n",
    "    fig = plt.figure(figsize=[14,14])\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=mercator)\n",
    "    \n",
    "    ax.set_extent([ -100, -102, -75.75, -74.25], ccrs.PlateCarree())\n",
    "    ax.quiver(lon,lat,u, v,units='xy', headwidth=3, transform=ccrs.PlateCarree())\n",
    "    geom = geometry.box(minx=-101.8,maxx=-100.5,miny=-75.25,maxy=-74.8)\n",
    "    ax.add_geometries([geom], crs=ccrs.PlateCarree(), facecolor='r', edgecolor='black', alpha=0.3)\n",
    "    ax.add_image(tiler, 6)\n",
    "    ax.coastlines()\n",
    "    ax.set_title('Quick Visualisation of {}'.format(varname)+'\\n for {}'.format(date), fontsize=20, fontweight='bold', pad=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e17156-73d9-481d-a09e-a70d8837d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_visualisation_ts(vardata, varname, time):\n",
    "    '''\n",
    "    Creates a timeseries plot for the variable of interest.\n",
    "    :param vardata: numpy array containing the variable time series data\n",
    "    :param varname: (string) the variable name for plotting\n",
    "    :param time: (string) the time period spanned by the variable time series data\n",
    "    '''\n",
    "    plt.style.use('seaborn-bright')\n",
    "    plt.figure(figsize=[20,10])\n",
    "    time = pd.DataFrame(time)\n",
    "    vardata = pd.DataFrame(vardata)\n",
    "    vardata_weekly = vardata.rolling(7).mean()\n",
    "    vardata_monthly = vardata.rolling(30).mean()\n",
    "    \n",
    "    plt.plot(vardata, color='k', linewidth=0.5, label='Daily MSLP')\n",
    "    plt.plot(vardata_weekly, color='b', linewidth=1.5, label='Weekly MSLP')\n",
    "    plt.plot(vardata_monthly, color='r', linewidth=1.5, label='Monthly MSLP')\n",
    "    \n",
    "    plt.title('Mean Sea Level Pressure over the Pine Island Polynya' , fontsize=22, fontweight='bold')\n",
    "    plt.xlabel('Time', fontsize=20)\n",
    "    plt.ylabel('Pressure (Pa)', fontsize=20)\n",
    "    #plt.xticks(ticks=time, rotation=60, fontsize=22)\n",
    "    plt.legend(fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c9254-1ccb-49f0-b6ce-2dfc64b3f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_date_index(date, time):\n",
    "    '''\n",
    "    Returns the index for the date of interest\n",
    "    :param date: date of interest\n",
    "    :param time: time data\n",
    "    '''\n",
    "    i = np.argwhere(time == date)\n",
    "    i = np.ravel(i)\n",
    "    return i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c4063-0d37-477e-acae-44bbacb91609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_mean(period, vardata, time, day=0):\n",
    "    '''\n",
    "    Takes the temporal mean of the variable data for the given period. For user friendliness, we can pick either 'weekly', 'monthly', 'yearly', 'overall' or 'seasonal' averages. \n",
    "    The 'seasonal' average will prompt the user to also include the 'season' of interest. For now, the function only calculates the temporal mean for the overall time period.\n",
    "    :param period: (string) time period of interest\n",
    "    :param vardata: numpy array containing variable data\n",
    "    :param time: numpy array containing time data\n",
    "    :param day: set to 0 if we are not extracting a single day and to 1 if we are.\n",
    "    :return: temporal mean of variable data for time period chosen\n",
    "    '''\n",
    "\n",
    "    if period == 'overall':\n",
    "        vardata_mn = np.mean(vardata, axis = 0)\n",
    "\n",
    "        \n",
    "    return vardata_mn[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579d6d2-508a-4700-8e42-42122e240c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_mean(vardata, lat, lon):\n",
    "    '''\n",
    "    Calculates the spatial mean of the data for the Polynya bounding box region.\n",
    "    '''\n",
    "    lat, lon, vardata_ = spatial_extent_reduction(lat, lon, vardata)\n",
    "    vardata_mn = np.mean(vardata_[:,0,:,:], axis = 2)\n",
    "    vardata_mn = np.mean(vardata_mn[:,:], axis=1)\n",
    "    \n",
    "    return vardata_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe80a1f-54a2-4620-b9d3-52e8c954ff7d",
   "metadata": {},
   "source": [
    "Options for variables, and corresponding units and variable names\n",
    "- Mean sea level pressure (Pa): 'msl'\n",
    "- Mean surface latent heat flux (W m**(-2)): 'mslhf'\n",
    "- Mean surface sensible heat flux (W m**(-2)): 'msshf'\n",
    "- Surface sensible heat flux (J m**(-2)): 'sshf'\n",
    "- Sea surface temperature (K): 'sst'\n",
    "- Zonal wind 10m above surface (m/s): 'u10'\n",
    "- Meridional wind 10m above surface (m/s): 'v10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efa9f9-d66b-4b36-b989-35fcd7f1efa3",
   "metadata": {},
   "source": [
    "Note: If you are unfamiliar with heat fluxes, a negative heat flux means heat is flowing from the surface to the atmosphere, a positive heat flux means heat is flowing from the atmosphere to the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3c964-5520-44a3-a492-5b87b80888c1",
   "metadata": {},
   "source": [
    "## Let's look at some variables of interest.\n",
    "### First let's set the time frame of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1057e-d1a8-4655-b8fd-ef2da372d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the period of interest to a specific day, weekly mean, monthly mean or yearly mean\n",
    "period = \"day\"\n",
    "if period == \"day\":\n",
    "    set_date = cftime.DatetimeGregorian(2019, 4, 15, 15, 0, 0, 0, has_year_zero=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907404d4-8b3f-4305-831b-8443cd254850",
   "metadata": {},
   "source": [
    "### Let's investigate the mechanical driving of the Polynya\n",
    "#### First we loop at the mean sea level pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dfae1-ec17-42d9-b188-efeaafd5357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the variable of interest\n",
    "var = 'msl'\n",
    "varname = 'Mean Sea Level Pressure (Pa)'\n",
    "time, lats, lons, vardata = load_data(var)\n",
    "\n",
    "#The following code isn't working and can't figure it out so will do this the non-modular way for now\n",
    "# Do some data extraction based on the time frame of interest\n",
    "#date_i, time_, vardata_ = temporal_mean(period, vardata, time, day= set_date)\n",
    "\n",
    "#extract index for date of interest set_date\n",
    "date_i = return_date_index(set_date, time)\n",
    "\n",
    "#plot the msl data for this day\n",
    "quick_visualisation(vardata[date_i,0,:,:], lons, lats, varname, time[date_i], filled_contours = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5422e-f444-41e9-b18a-03efb7aee543",
   "metadata": {},
   "source": [
    "#### Which can be further interpreted by the wind vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd9744-79b8-4818-9b58-edefbe3871be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set wind vector variable names\n",
    "var_u = 'u10'\n",
    "var_v = 'v10'\n",
    "varname = '10m Winds'\n",
    "#extract wind data\n",
    "time, lats, lons, vardata_u = load_data(var_u)\n",
    "time, lats, lons, vardata_v = load_data(var_v)\n",
    "#visualize wind data\n",
    "quick_visualisation_winds(vardata_u[date_i,0,:,:], vardata_v[date_i,0,:,:], lons, lats, varname, time[date_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4fa86-e5d7-408b-b346-43a44cbc99bb",
   "metadata": {},
   "source": [
    "### Now we can look at some energy fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc32d63-6ffc-4332-8778-8244e2a3c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the heat flux variable names of interest\n",
    "lh = 'mslhf'\n",
    "sh = 'msshf'\n",
    "\n",
    "lhname = 'Mean Surface Latent Heat Flux (W m**(-2))'\n",
    "shname = 'Mean Surface Sensible Heat Flux (W m**(-2))'\n",
    "\n",
    "# Load the data for the variables of interest\n",
    "time, lats, lons, lhdata = load_data(lh)\n",
    "time, lats, lons, shdata = load_data(sh)\n",
    "\n",
    "#return index for date of interest, set_date\n",
    "date_i = return_date_index(set_date, time)\n",
    "\n",
    "#calculate the net heat flux from the sensible and latent heat fluxes\n",
    "nethf = (lhdata[date_i,0,:,:] + shdata[date_i,0,:,:])\n",
    "nethfname = 'Net Mean Surface Heat Flux (W m**(-2))'\n",
    "\n",
    "#visualize the results\n",
    "quick_visualisation(lhdata[date_i,0,:,:], lons, lats, lhname, time[date_i], filled_contours = 1)\n",
    "quick_visualisation(shdata[date_i,0,:,:], lons, lats, shname, time[date_i], filled_contours = 1)\n",
    "quick_visualisation(nethf, lons, lats, nethfname, time[date_i], filled_contours = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34dad63-d39c-4c5a-9790-d6022f067732",
   "metadata": {},
   "source": [
    "### So it seems like the surface heat flux variables do not necessarily capture the Polynya\n",
    "#### Let's investigate whether any features are captured on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e4f45-b193-4abd-b96c-ad8d818bf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the heat flux variable names of interest\n",
    "lh = 'mslhf'\n",
    "sh = 'msshf'\n",
    "\n",
    "lhname = 'Mean Surface Latent Heat Flux (W m**(-2))'\n",
    "shname = 'Mean Surface Sensible Heat Flux (W m**(-2))'\n",
    "\n",
    "# Load the data for the variables of interest\n",
    "time, lats, lons, lhdata = load_data(lh)\n",
    "time, lats, lons, shdata = load_data(sh)\n",
    "\n",
    "# calculate the overall temporal mean for the data\n",
    "lhdata_mn = temporal_mean('overall', lhdata, time, day=0)\n",
    "shdata_mn = temporal_mean('overall', shdata, time, day=0)\n",
    "\n",
    "#calculate the net heat flux from the sensible and latent heat fluxes\n",
    "nethf_mn = (lhdata_mn[:,:] + shdata_mn[:,:])\n",
    "nethfname = 'Net Mean Surface Heat Flux (W m**(-2))'\n",
    "\n",
    "#visualize the results\n",
    "quick_visualisation(lhdata_mn, lons, lats, lhname, 'Mean for January 2018 - March 2022', filled_contours = 1)\n",
    "quick_visualisation(shdata_mn, lons, lats, shname, 'Mean for January 2018 - March 2022', filled_contours = 1)\n",
    "quick_visualisation(nethf_mn, lons, lats, nethfname, 'Mean for January 2018 - March 2022', filled_contours = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22dfb21-75a8-4afc-b531-0440e224f489",
   "metadata": {},
   "source": [
    "#### So it's become obvious that while the ERA5 data may help us understand some of the physical mechanisms that contribute to the formation of the Polynya, it cannot resolve the heat flux dynamics over the region. An extension would be to look at the time series of heat flux in the Polynya region and see if the behaviour of the time series mimicks the observed behaviour of the Polynya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d02dbb-61fc-474e-8a41-7522af6ba337",
   "metadata": {},
   "source": [
    "### Given that the ERA5 sea level pressure and winds are more interpretable, we can look at the time series for the Polynya region and assess whether the behaviour mimicks what is observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a85917-fa3d-4d8e-acca-5e176605322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the variable of interest\n",
    "var = 'msl'\n",
    "varname = 'Mean Sea Level Pressure (Pa)'\n",
    "time, lats, lons, vardata = load_data(var)\n",
    "vardata_mn = spatial_mean(vardata, lats, lons)\n",
    "\n",
    "quick_visualisation_ts(vardata_mn, varname, time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f4ea6-1455-4a7e-9c22-4dfa66869bc1",
   "metadata": {},
   "source": [
    "## Now let's move on to the three dimensional plots with ICESat-2 Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19341b-49b5-420c-b2f0-45984c002640",
   "metadata": {},
   "source": [
    "Here we define the dataCollector class based off of Philipp Arndt's code seen in visualisation tutorial. We define it within the code since there were issues importing the class from an outside python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f141c-dfb5-49db-9b3f-93b10a8bd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataCollector:\n",
    "    def __init__(self, beam=None, oaurl=None, track=None, date=None, latlims=None, lonlims=None, verbose=False):\n",
    "        if (beam is None) or ((oaurl is None) and (None in [track, date, latlims, lonlims])):\n",
    "            raise Exception('''Please specify a beam and \n",
    "            - either: an OpenAltimetry API url, \n",
    "            - or: a track, date, latitude limits and longitude limits.''')\n",
    "        else:\n",
    "            if oaurl is not None:\n",
    "                url = oaurl\n",
    "                tofind = '&beamName='\n",
    "                ids = url.find(tofind)\n",
    "                while ids>-1:\n",
    "                    url = url.replace(url[ids:ids+len(tofind)+4],'')\n",
    "                    ids = url.find(tofind)\n",
    "                iprod = url.find('/atl')\n",
    "                url = url.replace(url[iprod:iprod+6],'/atlXX')\n",
    "                url += tofind + beam + '&client=jupyter'\n",
    "\n",
    "                idate = url.find('date=') + len('date=')\n",
    "                date = url[idate:idate+10]\n",
    "                itrack = url.find('trackId=') + len('trackId=')\n",
    "                trackend = url[itrack:].find('&')\n",
    "                track = int(url[itrack:itrack+trackend])\n",
    "                bb = []\n",
    "                for s in ['minx=', 'maxx=', 'miny=', 'maxy=']:\n",
    "                    ids = url.find(s) + len(s)\n",
    "                    ide = url[ids:].find('&')\n",
    "                    bb.append(float(url[ids:ids+ide]))\n",
    "                lonlims = bb[:2]\n",
    "                latlims = bb[2:]\n",
    "            elif None not in [track, date, latlims, lonlims]:\n",
    "                url = 'https://openaltimetry.org/data/api/icesat2/atlXX?'\n",
    "                url += 'date={date}&minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={track}&beamName={beam}'.format(\n",
    "                        date=date,minx=lonlims[0],miny=latlims[0],maxx=lonlims[1],maxy=latlims[1],track=track,beam=beam)\n",
    "                url += '&outputFormat=json&client=jupyter'\n",
    "            \n",
    "            self.url = url\n",
    "            self.date = date\n",
    "            self.track = track\n",
    "            self.beam = beam\n",
    "            self.latlims = latlims\n",
    "            self.lonlims = lonlims\n",
    "            if verbose:\n",
    "                print('OpenAltimetry API URL:', self.url)\n",
    "                print('Date:', self.date)\n",
    "                print('Track:', self.track)\n",
    "                print('Beam:', self.beam)\n",
    "                print('Latitude limits:', self.latlims)\n",
    "                print('Longitude limits:', self.lonlims)\n",
    "                \n",
    "    def requestData(self, verbose=False): \n",
    "        if verbose:\n",
    "            print('---> requesting ATL03 data...',end='')\n",
    "        product = 'atl03'\n",
    "        request_url = self.url.replace('atlXX',product)\n",
    "        data = requests.get(request_url).json()\n",
    "        lat, lon, h, confs = [], [], [], []\n",
    "        for beam in data:\n",
    "            for confidence in beam['series']:\n",
    "                for p in confidence['data']:\n",
    "                    confs.append(confidence['name'])\n",
    "                    lat.append(p[0])\n",
    "                    lon.append(p[1])\n",
    "                    h.append(p[2])\n",
    "        self.atl03 = pd.DataFrame(list(zip(lat,lon,h,confs)), columns = ['lat','lon','h','conf'])\n",
    "        if verbose:\n",
    "            print(' Done.')\n",
    "            \n",
    "            print('---> requesting ATL06 data...',end='')\n",
    "        product = 'atl06'\n",
    "        request_url = self.url.replace('atlXX',product)\n",
    "        data = requests.get(request_url).json()\n",
    "        self.atl06 = pd.DataFrame(data['series'][0]['lat_lon_elev'], columns = ['lat','lon','h'])\n",
    "        if verbose:\n",
    "            print(' Done.')\n",
    "            \n",
    "            print('---> requesting ATL07 data...',end='')\n",
    "        product = 'atl07'\n",
    "        request_url = self.url.replace('atlXX',product)\n",
    "        data = requests.get(request_url).json()\n",
    "        self.atl07 = pd.DataFrame(data['series'][0]['lat_lon_elev'], columns = ['lat','lon','h'])\n",
    "        if verbose:\n",
    "            print(' Done.')\n",
    "            \n",
    "            print('---> requesting ATL08 data...',end='')\n",
    "        product = 'atl08'\n",
    "        request_url = self.url.replace('atlXX',product)\n",
    "        data = requests.get(request_url).json()\n",
    "        self.atl08 = pd.DataFrame(data['series'][0]['lat_lon_elev_canopy'], columns = ['lat','lon','h','canopy'])\n",
    "        if verbose:\n",
    "            print(' Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f300738-1cf3-4495-ac2a-46e9ad03f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "except: \n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a2647-b468-4918-8fec-31e40a98347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_oa_url(url,gtx):\n",
    "    mydata = dataCollector(oaurl=url,beam=gtx)\n",
    "    mydata.requestData()\n",
    "    return (mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc3df1-6863-4e75-b2c2-e023f510a3f5",
   "metadata": {},
   "source": [
    "We define a function to extract the satellite data we are interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc294a-d343-44cf-ae8c-d358a3233c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define url and gtx we desire\n",
    "PIG_20201007_url = 'http://openaltimetry.org/data/api/icesat2/atl08?date=2020-10-07&minx=-101.06993207685464&miny=-75.06319452393107&maxx=-100.2946192252&maxy=-74.74165227279721&trackId=210&beamName=gt3r&beamName=gt3l&beamName=gt2r&beamName=gt2l&beamName=gt1r&beamName=gt1l&outputFormat=json'\n",
    "PIG_20201007_gtx = 'gt2r'\n",
    "#download the data \n",
    "PIG_20201007_data = data_from_oa_url(PIG_20201007_url, PIG_20201007_gtx)\n",
    "#extract the data for ATL06 including, longitude, latitude and height\n",
    "atl06_data = PIG_20201007_data.atl06\n",
    "atl06_lat = atl06_data['lat']\n",
    "atl06_lon = atl06_data['lon']\n",
    "atl06_h = atl06_data['h']\n",
    "\n",
    "#define a meshgrid for the region\n",
    "new_lons, new_lats = np.meshgrid(atl06_lon, atl06_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9fed6-db92-4e7d-aed4-df7bbc144c44",
   "metadata": {},
   "source": [
    "Given that we want to visualize this data together with the ERA5 data, we also load the ERA5 data for mean sea level pressure for the same date and interpolate it to the same grid size to be able to more easily code a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc713f3-b682-481b-96ff-285fe4b00cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'msl'\n",
    "varname = 'Mean Sea Level Pressure (Pa)'\n",
    "time, lat, lon, vardata = load_data(var)\n",
    "old_lon, old_lat = np.meshgrid(lon, lat)\n",
    "\n",
    "\n",
    "date_i = return_date_index(set_date, time)\n",
    "\n",
    "vardata_old = vardata[date_i,0,:,:]\n",
    "time_ = time[date_i]\n",
    "\n",
    "#interp\n",
    "new_grid=griddata((old_lon.flatten(),old_lat.flatten()),vardata_old.flatten() , (new_lons,new_lats),method='cubic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816fe39-bd42-4d12-be04-9caa5179f1de",
   "metadata": {},
   "source": [
    "#### The rest is a work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee42d16-0ff6-4e76-a00d-265f9bbe5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig    = plt.figure()\n",
    "ax     = fig.gca(projection='3d')\n",
    "\n",
    "#x      = np.linspace(0, 1, 100)\n",
    "#X, Y   = np.meshgrid(x, x)\n",
    "#levels = np.linspace(-0.1, 0.4, 100)  #(z_min,z_max,number of contour),\n",
    "\n",
    "#a=0\n",
    "#b=1\n",
    "#c=2\n",
    "#Z1 = a+.1*np.sin(2*X)*np.sin(4*Y)\n",
    "#Z2 = b+.1*np.sin(3*X)*np.sin(4*Y)\n",
    "#Z3 = c+.1*np.sin(4*X)*np.sin(5*Y)\n",
    "\n",
    "#plt.contourf(old_lon, old_lat, vardata_old, levels =100, zdir='z',cmap=plt.get_cmap('nipy_spectral'))\n",
    "#line_c = plt.contour(old_lon, old_lat, vardata_old, colors=['black'])\n",
    "#ax.clabel(\n",
    "#        line_c,  # Typically best results when labelling line contours.\n",
    "#        colors=['black'],\n",
    "#        manual=False,  # Automatic placement vs manual placement.\n",
    "#        inline=True,  # Cut the line where the label will be placed.\n",
    "#        fmt=' {:.0f} '.format,  # Labes as integers, with some extra space.\n",
    "#    )\n",
    "#plt.p\n",
    "\n",
    "plt.contourf(new_lons, new_lats, new_grid, levels =100, cmap=plt.get_cmap('nipy_spectral'))\n",
    "#line_c = plt.contour(new_lons, new_lats, new_grid, colors=['black'])\n",
    "#plt.clabel(\n",
    "#        line_c,  # Typically best results when labelling line contours.\n",
    "#        colors=['black'],\n",
    " #       manual=False,  # Automatic placement vs manual placement.\n",
    "  #      inline=True,  # Cut the line where the label will be placed.\n",
    "   #     fmt=' {:.0f} '.format,  # Labes as integers, with some extra space.\n",
    "    #)\n",
    "#plt.plot(atl06_lon, atl06_lat, atl06_h, 'red')\n",
    "#plt.contourf(X, Y,Z3, levels=c+levels,cmap=plt.get_cmap('rainbow'))\n",
    "#ax.set_xlim3d(0, 1)\n",
    "#ax.set_ylim3d(0, 1)\n",
    "#ax.set_zlim3d(0, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537b33-c4ae-41f0-9157-fe21171e09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(atl06_lon, atl06_lat, atl06_h, 'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df90fe-e153-41b2-8bae-3c6e3898d136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
